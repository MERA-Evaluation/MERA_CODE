{
    "Task description": "**RealCode** is a benchmark for evaluating the ability of language models to generate function bodies in real-world Python repositories. The benchmark focuses on realistic completions using project-level context and validates correctness through test execution.",
    "Motivation": "This dataset tests how well models can:\n- Generate function bodies based on surrounding code context;\n- Integrate into existing Python projects;\n- Pass real unit tests after insertion.\nThe main evaluation metric is `pass@k`, computed via execution of repository-specific tests inside Docker containers.",
    "Dataset creation": "The benchmark is built from 95 public Python GitHub repositories created in 2024. There are 802 tasks in total: for each sample, a function is extracted along with its surrounding code (`left_context`) and evaluated based on whether the generated body passes original unit tests. All examples come from real repositories and are reproducibly executable.",
    "Contributors": "Pavel Zadorozhny, Rodion Levichev, Pavel Adamenko, Aidar Valeev, Dmitrii Babaev, Denis Kokosinskiy"
  }
