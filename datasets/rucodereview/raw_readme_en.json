{
    "Task description": "The task is to automatically generate code review comments in Russian based on code changes.\nThe dataset includes 699 examples of code changes (in Java, Python, Scala, and Go) paired with corresponding comments. All comments have been verified for reproducibility (ability to reconstruct context from code diffs) and relevance to the changes.",
    "Motivation": "This benchmark addresses a critical gap in evaluating multilingual code-aware LLMs. Below is a detailed rationale:\nTarget Models:\nMultilingual LLMs capable of processing the Russian language and analyzing code syntax/semantics.\nExcludes models that: Lack Russian language support or cannot interpret code diffs.\nUsers: Developers and researchers focused on automating code review pipelines for Russian-speaking teams.\nEvaluated Capabilities:\nUnderstanding syntactic and semantic code changes.\nGenerating issue-specific comments (e.g., bugs, optimizations, style violations).\nMultilingual code support (Java, Python, Scala, Go).\nThese capabilities are critical for integrating LLMs into real-world development pipelines, where precision and compliance with standards are non-negotiable.\nDataset Design\nComments are anchored to specific code changes to eliminate reliance on external context.\nTwo-stage filtering (LLM + dual human review) ensures comments are self-contained and reproducible.\nValidity is enforced via strict criteria: Comments must be interpretable solely from the attached code changes.\nMetrics:\nBLEU/ChrF: Evaluate textual alignment with the reference.\npass@k:\nMeasures the percentage of cases where at least one of the first k generated comments is deemed correct via LLM-as-a-Judge evaluation.\nThe model generates up to 10 comment variants per code change.\nEach variant is validated by an LLM judge for semantic consistency with the reference comment (reference-based).\nA sample is considered successful if at least one of the first k variants passes validation.\nWhy These Metrics?\nWe do not require exact replication of the reviewer\u2019s comment.\nInstead, we assess the model\u2019s ability to generate generally high-quality comments that align with human judgment.\nCritical for code review tasks, where multiple valid approaches exist to highlight issues.\n",
    "Dataset creation": "The research analyzed repositories created in the BackEnd Academy during 2024. Key stages included:\n1. Data Collection: Pull request comments, commit messages, and code line annotations were extracted via the GitHub API.\n2. Code Segmentation: Language-specific parsers (AST for Java/Scala, go/parser for Go, Python\u2019s ast) identified logical code blocks (classes, functions).\n3. Dataset Filtering:\n   - Removed comments targeting non-relevant programming language files (50 samples excluded).\n   - GPT-4o classification into reproducible/non-reproducible: Assessed whether a comment\u2019s context could be inferred solely from code changes.\n   - Human review of 1300 samples pre-labeled as \"reproducible\": 699 samples approved.",
    "Human baseline": "Human baseline evaluation methodology.",
    "Contributors": "Artem Zavgorodnev, Aleksandr Medvedev, Georgiy Mkrtchyan, Ivan Kharkevich, Ilseyar Alimova, Nikolay Bushkov, Stanislav Moiseev."
}