{
    "Task description": "Long context code generation on C/C++ at function level",
    "Motivation": "YABLoCo is a long context code generation benchmark featuring a test set of 208 functions selected from four large repositories with\nthousands of functions. The dataset contains metadata of functions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. The benchmark aims at function body generation in large repositories\nfrom 200K to 2,000K LoC in C and C++, two languages not covered by previous benchmarks. While the benchmark generally allows custom\nretrieval across repositories, the provided version proposes oracle context -- functions extracted from call graph that generated code\nshould depend on. Given oracle context, docstring and function signature, model generates correspondin function body, which then gets\nevaluated on repository tests. Model should understand code from provided context and docstring summary and compose a function method\nthat implements the required functionality. Evaluation presents results in two metrics: pass@1, measuring functionality correctness,\nand exact match, which indicates overfitting in case of high values.",
    "Dataset creation": "Largest most-starred selected GitHub repositories included llvm-project, bullet3, openssl and redis. The limited number of repositories is due\nto high costs of including more repositories, specifically, building and compiling projects, implementing Dockerfiles, running tests and \ncomputing test coverage. From each of the selected repositories, we extracted all functions along with their function calls, last commit date,\ndocstring comment, code and comment length, and test hits. The function calls were then assigned to one of the following five categories: 'none',\n'stdlib', 'file', 'package', 'project'. Specifically, 'stdlib' for system calls, 'file' and 'package' for calls inside the same file and package,\ncorrespondingly, and 'project' for functions with project-level calls. If a function had no dependencies, it went into the 'none' category. We\nfiltered out the functions that had excessively short or long implementations, or no test hits or comments. Then, we detected and removed near\ncode duplicates. After that, we sorted the remaining set of functions in every context category according to the last commit date and test hits\npreferring the latest and most covered. The repositories functions were sampled automatically disregarding the docstring quality. Therefore, we\nmanually evaluated the docstring quality. In addition to the data collection and cleaning, we generated a call graph for each repository. The\ngraph contained all functions with unique IDs, their callers, and callee functions as well as metadata such as length, path to file, position\nin file, docstring, date of the last modification, number of test hits, and a category.",
    "Contributors": "Aidar Valeev, Roman Garaev, Vadim Lomshakov, Irina Piontkovskaya, Vladimir Ivanov, Israel Adewuyi"
}