{"Task description": "Evaluation of correct code generation based on incorrect code and a list of errors or warnings from the linter.", "Motivation": "\nThe aim of benchmark is identifying the capabilities of models for generating and correcting code based on linter errors.\nThe dataset is focused on:\n- Code-generating models (e.g. Codex, CodeLlama, StarCoder, GPT-4) that can correct code based on linter errors.\n- Models that specialize in refactoring and code correction (e.g. DeepSeek-R1, CodeT5).\n- Multimodal models, if they can work with code and text instructions.\nNot suitable for:\n- Models without code understanding (e.g. purely text LLMs without additional training on code).\n- Models that do not support Python.\n- Models fine-tuned to solve the FIM (Fill-in-the-middle) problem.\n\nThe evaluation results may be useful for:\n- Developers of tools for automatic code refactoring (e.g. IDE plugins);\n- Researchers in the field of code generation and correction;\n- Engineers evaluating the quality of code generation models.\n\nThe benchmark results will show how well the model copes with fixing code based on linter errors, which means that the model\ncan be useful for:\n- automatic code fixing,\n- improving code quality in IDEs,\n- teaching beginners to write \"clean\" code.\nIf the model does not cope well, it means that it either does not understand linter errors, or does not know how to apply fixes correctly.\n\nThe benchmark evaluates:\n1. Understanding linter errors - the ability to correctly interpret messages like E111, E231, etc.\n2. Correct code refactoring - the ability to make corrections while preserving the logic of the program.\n3. Following the code style (PEP 8) - correct indents, spaces, formatting.\n4. Contextual understanding of the code - the model should not break the logic when fixing the style. Linter errors are a common occurrence in development, and automatic correction saves time.\nIf the model cannot correct simple style errors, it is unlikely to cope with more complex refactoring tasks.\n\nThe dataset contains:\ncode – source code with errors;\nfeedback – list of errors from the linter;\ninstruction – explicit instruction to correct the code based on feedback;\ncanonical_code – reference code\nExplicit indication of errors allows us to evaluate the model's ability to correct code according to the linter, rather than \"guess\" errors.\nThe canonical solution (canonical_code) provides a clear ground truth for evaluation.\nThe instruction explicitly specifies the task so that the model does not deviate from the goal.\n\nIf the model corrects the code according to the feedback and the linter does not detect errors after checking the generation results, then the result is correct.\nIf errors persist or new ones appear, the model does not solve the problem.\n\nThe metric is pass@k, determined based on the success of the linter check relative to the total size of the dataset.\n", "Dataset creation": "\nTo create a high-quality dataset, where the model should correct the code based on linter errors, it is necessary\n1) A subset of the mbpp dataset was selected as the initial dataset.\n2) For each code example, the flake8 linter was run to detect errors.\n3) Only those examples that had at least 1 error were included in the final dataset.\n4) To obtain canonical solutions, the code was manually corrected by experts and re-checked by the linter.\n5) The results of the linter check are saved as lines with the error code and its description from the linter and saved in the feedback field\n(i.e., E111 indentation is not a multiple of four, E231 missing whitespace after ',')\n", "Human baseline": "The human_baseline methodology is in development.", "Contributors": "Vikulov V.A., Bykov A.V., Pikhtovnikov K.E."}