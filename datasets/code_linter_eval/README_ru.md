# CodeLinterEval


## Описание задачи

Оценка генерации корректного кода на основе некорректного кода и списка ошибок/предупреждений от линтера.

Тестируемые навыки моделей: Instruction following, Perception(Python), Code changing

Авторы: Владимир Викулов, Антон Быков, Кирилл Пихтовников


## Мотивация

Бенчмарк направлен на выявление способностей моделей для генерации и исправление кода на основе ошибок от линтера.
Датасет ориентирован на:  
- Кодогенерирующие модели (например, Codex, CodeLlama, StarCoder, GPT-4), способные исправлять код на основе линтерных ошибок.  
- Модели, специализирующиеся на рефакторинге и исправлении кода (например, DeepSeek-R1, CodeT5).
- Мультимодальные модели, если они умеют работать с кодом и текстовыми инструкциями.  
Не подходит для:  
- Моделей без понимания кода (например, чисто текстовые LLM без дообучения на code).  
- Моделей, не поддерживающих Python.  
- Моделей, тонко настроенных на решение задачи FIM(Fill-in-the-middle).

Результаты оценки могут быть полезны:  
- Разработчикам инструментов для автоматического рефакторинга кода (например, IDE-плагины);  
- Исследователям в области генерации и исправления кода;  
- Инженерам, оценивающим качество code generation моделей.  

Результаты бенчмарка покажут насколько хорошо модель справляется с исправлением кода по линтерным ошибкам, что означает, что модель 
может быть полезна для:  
  - автоматического исправления кода,  
  - улучшения качества кода в IDE,  
  - обучения новичков писать "чистый" код.  
Если модель плохо справляется, значит, она либо не понимает линтерные ошибки, либо не умеет правильно применять исправления. 

В бенчмарке оценивается:  
1. Понимание линтерных ошибок – способность правильно интерпретировать сообщения типа E111, E231 и т.д.  
2. Корректный рефакторинг кода – умение вносить исправления, сохраняя логику программы.  
3. Следование стилю кода (PEP 8) – правильные отступы, пробелы, форматирование.  
4. Контекстное понимание кода – модель не должна ломать логику при исправлении стиля.
Линтерные ошибки – частый случай в разработке, и автоматическое исправление экономит время.
Если модель не умеет исправлять простые стилистические ошибки, она вряд ли справится с более сложными задачами рефакторинга.  

Если модель исправляет код в соответствии с feedback и линтер не обнаруживает ошибок после проверки результатов генерации - значит результат корректен.
Если ошибки сохраняются или появляются новые - модель не решает поставленную задачу.

Метрикой является pass@k, определяемая на результатах успешности проверки линтером относительно общего объема датасета.


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели. Могут включать одну или несколько модальностей - видео, аудио, изображение, текст.
    - `code` [str] — Строка, содержащая код на Python с ошибками.
    - `feedback` [str] — Строка, содержащая описание ошибок от линтера.
- `outputs` [str] — Одномерный массив строк размера n_samples, где n_samples - количество сэмплов, требуемое для подсчета метрики pass@k.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `canonical_code` — каноническое решение задачи (код без ошибок/предупреждений линтера) задания

### Пример данных

```json
{
	"inputs": {
		"code": "import re\n\ndef find_literals(text, pattern):\n  match = re.search(pattern, text)\n  s = match.start()\n  e = match.end()\n  return (match.re.pattern, s, e)",
		"feedback": "E302: expected 2 blank lines, found 1 in 3 line\nE111: indentation is not a multiple of 4 in 4 line\nE111: indentation is not a multiple of 4 in 5 line\nE111: indentation is not a multiple of 4 in 6 line\nE111: indentation is not a multiple of 4 in 7 line\nW292: no newline at end of file in 7 line\n"
	},
	"instruction": "Перепиши код с учетом ошибок полученных от линтера. \nКод: {code} \nОшибки от линтера:{feedback}",
	"meta": {
		"canonical_code": "\nimport re\n\n\ndef search_regex(pattern, string):\n    match = re.search(pattern, string)\n    if match:\n        return match.group(), match.start(), match.end()\n    else:\n        return None\n",
		"id": 5
	},
	"outputs": [
		1,
		1,
		1,
		1,
		1,
		1,
		1,
		1,
		1,
		1
	]
},
```


### Промпты

Для задачи были подготовлены 10 промптов, которые были равномерно распределены по вопросам по принципу "один вопрос – один промпт". Шаблоны в фигурных скобках в промпте заполняются из полей внутри поля `inputs` в каждом вопросе.


Пример промпта:

```
prompt_0
```


### Создание датасета

Для создания качественного датасета, где модель должна исправлять код на основе линтерных ошибок, необходимо:  
1. В качестве исходного датасета было выбрано подмножество из датасета mbpp.  
2. Для каждого примера кода был запущен линтер flake8 для выявления ошибок.  
3. В итоговый датасет попали только те примеры, которые имели не менее 1 ошибки.
4. Для получения канонических решений код был вручную исправлен экспертами и повторно проверен линтером.  
5. Результаты проверки линтером сохранены в виде строк с кодом ошибки и ее описанием от линтера и сохранены в поле feedback (например, 
"E111 indentation is not a multiple of four, E231 missing whitespace after ','")


## Оценка


### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `pass@k`: Метрика pass@k измеряет долю тестовых случаев, которые программа проходит, от общего количества тестовых случаев


### Human baseline

В разработке
