{
    "Task description": "Evaluation of unit-test generation for functions and methods in five programming languages (Java, Python, Go, JavaScript, and C#).",
    "Motivation": "Unit testing is an important software development practice in which individual components of a software system are evaluated in isolation.\nThis benchmark is aimed at evaluating the ability of models to generate unit tests for methods and functions in five programming languages - Java, Python, Go, JavaScript and C#.\n\nThe task of unit-test generation is formulated as follows: there is a function or method (called focal function/method), and we need to generate a unit-test for it (test function/method).\n\nThe dataset is focused on instructional code and multimodal models.\n\nThe evaluation results may be useful:\n- for researchers in the field of automatic code generation in general and unit tests in particular\n- developers of tools for automatic code generation\n\nBased on the results, it will be possible to conclude how similar the tests generated by the model are to tests written by humans.\n\nThe task evaluates the ability of the model to generate unit tests for a given function/method, taking into account the additional context gathered from the project. \nThus, the ability to generate code at the project level is also tested, which is important when using models to generate unit tests for real projects.\n\nThe model is provided with instructions that contain\n- task (generate unit test)\n- programming language used\n- the text of the method/function for testing\n- the path to the file of the method/function for testing, as well as the rest of the code from this file\n- the path to the file where the generated test function will be located\n- (optional) a test framework that needs to be used\n- type of a unit test to generate - method or function\n- additional context from a future test file\n\nAbout the additional context from the test file:\n\nSince the dataset was gathered from real repositories files from github, the test project files contain more than one test function, and there may also be necessary imports, variables, auxiliary functions and methods, as well as the test functions for different units. \nIn the basic scenario, we can simply give the model a unit (and even some additional context for this unit) and ask to generate a test for it. \nHowever, in this case, the problem appears that the model did not see the context from the test file. \nIf such data is used for training, then we intentionally teach the model to hallucinate, namely, to use some libraries, other functions and classes in the test function that have not been described anywhere before. \nIf we use such data for testing, it turns out that we have provided the model with a very limited context and the comparison with human tests is not honest.\nYou could simply give the model the rest of the text from the test file and ask it to generate a targeted test function, but then there is a high probability of data leakage - this test function could be used somewhere in the file, and other test functions could also fall into such a context. \nTherefore, we decided to collect some cutted context for each test function from the test files for the model input. \nThe content of such a context depends on the programming language and is described in the table below.\n\n|            | test file context                                                                                                                                                                                          |\n|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Java       | imports, test class definition, all class fields, constructors or setUp/tearDowm methods (full body), other public non-test methods signatures,  remove all comments                                       |\n| Python     | imports, global code,  non-test class definitions, not-test function signatures, for methods - short class definition  without test methods                                                                |\n| Go         | package, imports,  all structures, global code, all non-test functions/methods signatures, TestMain function (full body)                                                                                   |\n| JavaScript | test framework name, imports, all code except describe(), test() and it() calls,  code for the parent describe() call  (which defines test suite)  with all the it() and test() calls cutted from the code |\n| C\\#        | using directives,  all global code (not class, method, function  or namespace declaration),  test class without test methods                                                                               |\n\nThe quality assessment metric is CodeBLEU, which evaluates the similarity of a generated test and a test written by a human.",
    "Dataset creation": "The dataset collection process consisted of the following steps: \n1. Parsing the repository list, filtering the list, and downloading repositories.\n2. Parsing repositories, functions, methods, and tests\n3. Comparison of methods/functions and their corresponding tests. \n\nThese steps will be described in more detail later.\n\nA list of repositories was downloaded using GitHub API for each language.\nWe chose the repositories with permissive licenses only and with the number of stars more than 10. We also filtered out fork repositories. The list of licenses used in the dataset: MIT License, Apache License 2.0, The Unlicense, Mozilla Public License 2.0, BSD 2-Clause \"Simplified\" License, BSD 3-Clause \"New\" or \"Revised\" License, EPL 1.0 license, EPL 2.0 license, MPL 2.0 License, Unlicense License, 0BSD license.\n\nWhen building the dataset, the same filtering rules for all languages were used: \n+ Empty tests are removed. \n+ No more than 200 method-test pairs were collected from one repository. If there were more pairs, they were sampled randomly. \n+ The test case should be less than 5000 characters. This limit is set to remove overly long tests from the data. \n+ Maximum input length (focal function with context) should be less than 70000 characters. \n+ Maximum number of assertions (the word \"assert\" in the test case) is 20. \n+ For Python and Java, there was additional filtering for tests with syntax errors (using ast and javalang libraries correspondingly). \n+ The training data was filtered for duplicates of test cases both within a set, and possible overlaps with the validation and test data were removed.\n\nTable below provides a description of the test frameworks of tests included in the dataset as well as a summary of how test functions were identified.\n\n| Language   | Test frameworks                                              | Test file and functions/methods definitions                                                                                                                             |\n|------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Java       | JUnit                                                        | methods with \"@Test\" annotation                                                                                                                                         |\n| Python     | pytest, unittest                                             | \"test\\_*.py\" and \"*\\_test.py\" files;  \"test\\_*\" functions and methods                                                                                                   |\n| Go         | testing                                                      | \"*\\_test.go\" files;  \"func TestXxx(t *testing.T)\" functions                                                                                                             |\n| JavaScript | Mocha, Jest,  Jasmine, QUnit,  Nightwatch,  other frameworks | \"*test\\*/\\*\\*/\\*.[t\\|j]s\" and \"\\*\\*/\\*[test\\|spec].[t\\|j]s\" files;  test functions defined inside it() and test()                                                 |\n| C\\#        | Nunit,  XUnit, MSTest                                        | methods with attributes: [Test], [TestCase], [TestCaseSource], [Theory] - for Nunit; [Fact], [Theory] - for XUnit;  [TestMethod] - for MSTest |\n\nFor all languages (except Python) the tree-sitter was used for\ncode parsing, specifically, for searching and parsing functions/meth-\nods and classes, identifying calls, etc. For Python, we use the built-in\nast library.\n\nAfter we have parsed all the classes, methods, and functions in the repository, we need to somehow understand which method/function the test functions are testing. In other words, we need to compare them and create a list of test-method pairs. Methods/functions and the unit-tests for them were mapped using the method adapted from the [paper](https://arxiv.org/abs/2009.05617) (only Java methods and tests were compared in the work). We will briefly describe below how we have adapted this method for each language.\n\n+ For Java, test classes are mapped with focal classes by their paths and names. Focal and test methods are then matched using two heuristics - names and unique method invocation.  \n+ For Python, all parsed functions and methods were mapped with tests in accordance with the test naming conventions in pytest. Based on the logic that one function can be tested by several tests, but one test is aimed at only one function, only tests matched with one function/method are added to the dataset.\n+ For Go, the identification of test functions and mapping them to focal functions was carried out following the naming conventions of the testing package. The test-mapping procedure was performed in the same way as for Python.\n+ For C\\#, focal and test methods are mapped if the name of the test method includes the name of a non-test method from the repository and makes an invocation of this method. Only tests mapped with one focal method are added to the dataset.\n+ For JavaScript, the test framework used in the repository was also parsed by searching for one of the following libraries in dependencies in \"package.json\" file: \"mocha\", \"jest\", \"jasmine\", \"qunit\", \"nightwatch\". Subsequently, the name of the framework was added to the model input as one of the parts of the test file context. Unlike other languages, this is necessary, since imports often do not contain information about the test framework. If a test framework from the list was not found in the repository dependencies, the test function was still added to the dataset, but the test framework was defined as \"Unknown\". As for method-test mapping, this is the only language where it was based only on the last local method/function invocation because test functions do not have identifiers when declared in it() and test().",
    "Contributors": "Alena Pestova, Valentin Malykh"
}