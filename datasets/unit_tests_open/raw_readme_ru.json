{
    "Описание задачи": "Оценка генерации юнит-тестов для функций и методов на пяти языках программирования (Java, Python, Go, JavaScript и C#).",
    "Мотивация": "Юнит-тестирование - это важная практика разработки программного обеспечения, при которой отдельные компоненты программной системы оцениваются изолированно.\nДанный бенчмарк направлен на оценку способностей моделей генерировать юнит-тесты для методов и функций на пяти языках программирования - Java, Python, Go, JavaScript и C#.\n\nЗадача генерации юнит-теста формулируется следующим образом: дана функция или метод (далее фокальная/тестируемая функция/метод), и необходимо сгенерировать юнит-тест для нее (далее - тестовая функция/метод/тест).\n\nДатасет ориентирован на инструктивные кодовые и мультимодальные модели.\n\nРезультаты оценки могут быть полезны:\n- исследователям в области автоматической генерации кода в целом и юнит-тестов в частности\n- разработчикам инструментов для автоматической генерации кода\n\nНа основании результатов можно будет сделать вывод о том, насколько сгенерированные моделью тесты схожи с тестами, написанными людьми.\n\nЗадача оценивает способность модели генерировать юнит-тесты для данной функции/метода, учитывая дополнительный контекст, собранный из проекта. Таким образом, также проверяется способность генерации кода на уровне проекта, что важно при использовании моделей для генерации юнит-тестов для реальных проектов.\n\nМодели предоставляется инструкция, которая содержит\n- задачу (сгенерировать юнит-тест)\n- используемый язык программирования\n- текст метода/функции для тестирования,\n- путь к файлу метода/функции для тестирования, а также остальной код из этого файла\n- путь к файлу, где будет лежать сгенерированная тестовая функция\n- (опционально) тестовый фреймворк, который необходимо использовать\n- тип юнит-теста для генерации - метод или функция\n- дополнительный контекст из как бы существующего будущего тестового файла\n\nПро дополнительный контекст из тестового файла:\n\nТак как датасет собирался из файлов реальных репозиториев с гитхаба, в тестовых файлах проектов содержится не одна тестовая функция, а также могут быть необходимые импорты, переменные, вспомогательные функции и методы, а также сами тестовые функции на разные юниты. Понятно, что в базовом сценарии мы можем просто дать модели юнит (и даже какой-то дополнительный контекст для этого юнита) и попросить сгенерировать для него тест. Однако в таком случае появляется проблема, что модель не видела контекста из тестового файла. Если такие данные используются для обучения, то мы намеренно учим модель галлюцинировать, а именно испоьзовать в тестовой функции какие-то библиотеки, другие функции и классы, которые нигде не были описаны ранее. Если мы используем такие данные для тестирования, то получается, что мы предоставили модели очень ограниченный контекст и сравнение с человеческими тестами не самое честное.\nМожно было бы просто давать модели весь остальной текст из тестового файла и просить сгенерировать таргетную тестовую функцию, но тогда появляется большая вероятность утечки данных - где-то в файле могла использоваться данная тестовая функция, к тому же другие тестовые функции тоже могли попасть в такой контекст. Поэтому нами было принято решение собирать из тестовых файлов некоторый урезанный контекст для каждой тестовой функции и передавать его модели. Содержание такого контекста зависит от языка программирования и описано в таблице ниже.\n\n|            | test file context                                                                                                                                                                                          |\n|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Java       | imports, test class definition, all class fields, constructors or setUp/tearDowm methods (full body), other public non-test methods signatures,  remove all comments                                       |\n| Python     | imports, global code,  non-test class definitions, not-test function signatures, for methods - short class definition  without test methods                                                                |\n| Go         | package, imports,  all structures, global code, all non-test functions/methods signatures, TestMain function (full body)                                                                                   |\n| JavaScript | test framework name, imports, all code except describe(), test() and it() calls,  code for the parent describe() call  (which defines test suite)  with all the it() and test() calls cutted from the code |\n| C\\#        | using directives,  all global code (not class, method, function  or namespace declaration),  test class without test methods                                                                               |\n\nМетрикой оценки качества является CodeBLEU, оценивающая схожесть сгенерированного теста и теста, написанного человеком.",
    "Создание датасета": "Процесс сбора датасета состоял из следующих этапов: \n1. Парсинг списка репозиториев, фильтрация списка и скачивание репозиториев.\n2. Парсинг репозиториев, функций, методов и тестов\n3. Сопоставление методов/функций и соответвующих им тестов. \n\nДалее будут подробнее описаны эти этапы.\n\nСписок репозиториев для каждого языка был получен с помощью GitHub API. Мы выбрали репозитории только с открытыми лицензиями и те, у которых больше 10 звезд. Мы также отфильтровали fork-репозитории. Список используемых лицензий: MIT License, Apache License 2.0, The Unlicense, Mozilla Public License 2.0, BSD 2-Clause \"Simplified\" License, BSD 3-Clause \"New\" or \"Revised\" License, EPL 1.0 license, EPL 2.0 license, MPL 2.0 License, Unlicense License, 0BSD license.\n\nПри построении набора данных использовались одинаковые правила фильтрации для всех языков: \n+ Пустые тесты удалены. \n+ Из одного репозитория было собрано не более 200 пар метод-тест. Если пар было больше, они отбирались случайным образом. \n+ Тестовый пример должен содержать не более 5000 символов. Это ограничение установлено для удаления из данных слишком длинных тестов. \n+ Максимальная длина входных данных (тестируемая функция с контекстом) должна составлять не более 70000 символов. \n+ Максимальное количество проверок (слово \"assert\" в тестовом примере) равно 20. \n+ Для Python и Java была реализована дополнительная фильтрация тестов с синтаксическими ошибками (с использованием библиотек ast и javalang соответственно). \n+ Обучающие данные были отфильтрованы на наличие дубликатов.\n\nВ таблице ниже приведено описание тестовых фреймворков тестов, включенных в набор данных, а также краткое описание того, как были определены тестовые функции.\n\n| Language   | Test frameworks                                              | Test file and functions/methods definitions                                                                                                                             |\n|------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Java       | JUnit                                                        | methods with \"@Test\" annotation                                                                                                                                         |\n| Python     | pytest, unittest                                             | \"test\\_*.py\" and \"*\\_test.py\" files;  \"test\\_*\" functions and methods                                                                                                   |\n| Go         | testing                                                      | \"*\\_test.go\" files;  \"func TestXxx(t *testing.T)\" functions                                                                                                             |\n| JavaScript | Mocha, Jest,  Jasmine, QUnit,  Nightwatch,  other frameworks | \"*test\\*/\\*\\*/\\*.[t\\|j]s\" and \"\\*\\*/\\*[test\\|spec].[t\\|j]s\" files;  test functions defined inside it() and test()                                                 |\n| C\\#        | Nunit,  XUnit, MSTest                                        | methods with attributes: [Test], [TestCase], [TestCaseSource], [Theory] - for Nunit; [Fact], [Theory] - for XUnit;  [TestMethod] - for MSTest |\n\nДля всех языков (кроме Python) для разбора кода использовался tree-sitter, в частности, для поиска и разбора функций/методов и классов, идентификации вызовов и т.д. Для Python мы использовали встроенную библиотеку ast.\n\nПосле того, как мы спарсили все классы, методы и функции в репозитории, необходимо как-то понять, какой именно метод/функцию тестируют тестовые функции. То есть необходимо их сопоставить и сформировать список метод-тест. Методы/функции и юнит-тесты для них были сопоставлены с использованием метода, адаптированного из [статьи](https://arxiv.org/abs/2009.05617) (в работе сопоставляли только методы и тесты на Java). Далее будет кратко описано, как мы адаптировали этот метод для каждого языка.\n\n+ В Java тестовые классы сопоставляются с фокальными классами по их путям в репозитории и именам. Затем методы фокального и тестового классов сопоставляются с помощью двух эвристик - имен и уникальности вызова метода.  \n+ Для Python все проанализированные функции и методы были сопоставлены с тестами в соответствии с правилами именования тестов в pytest. Исходя из логики, согласно которой одна функция может быть протестирована несколькими тестами, но один тест предназначен только для одной функции, в набор данных добавляются только тесты, соответствующие одной функции/методу.\n+ Для Go идентификация тестовых функций и сопоставление их с фокальными функциями были выполнены в соответствии с практиками именования тестовых файлов и функций в библиотеке testing. Процедура сопоставления тестов была выполнена таким же образом, как и для Python.\n+ Для C\\# фокальные и тестовые методы объединяются в пару, если название тестового метода включает в себя название фокального метода из репозитория и вызывает этот метод. В набор данных добавляются только тесты, сматченные с одним фокальным методом.\n+ Для JavaScript также был проанализирован тестовый фреймворк, используемый в репозитории, путем поиска одной из следующих библиотек в зависимостях в файле \"package.json\": \"mocha\", \"jest\", \"jasmine\", \"qunit\", \"nightwatch\". Впоследствии название фреймворка было добавлено во инпут модели как одна из частей контекста тестового файла. В отличие от других языков, это необходимо, поскольку импорт часто не содержит информации о тестовом фреймворке. Если тестовый фреймворк из списка не был найден в зависимостях репозитория, тестовая функция все равно добавлялась в набор данных, но тестовый фреймворк определялся как \"Unknown\". Что касается сопоставления метода и теста, то это единственный язык, где оно было основано только на последнем вызове локального метода/функции, поскольку тестовые функции не имеют идентификаторов при объявлении в it() и test().",
    "Авторы": "Алена Пестова, Валентин Малых"
}